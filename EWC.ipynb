{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adamax\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test, batch_size=128):\n",
    "  acc = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "  # 배치 단위로 데이터를 처리\n",
    "  for i in range(0, len(X_test), batch_size):\n",
    "      # 배치 데이터 분리\n",
    "      direction = X_test[i:i + batch_size]\n",
    "      labels = y_test[i:i + batch_size]\n",
    "        \n",
    "      # 모델 예측\n",
    "      preds = model.predict_on_batch(direction)\n",
    "        \n",
    "      # 정확도 갱신\n",
    "      acc.update_state(labels, preds)\n",
    "    \n",
    "    # 최종 정확도 반환\n",
    "  return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_matrices(model, X_train, y_train, num_batches=10, batch_size=128):\n",
    "  \n",
    "  # 데이터를 배치로 나누기\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))  # Feature와 Label을 TensorFlow Dataset으로 묶음\n",
    "  task_set = dataset.batch(batch_size).repeat()  # 배치로 나누고 반복 가능하게 설정\n",
    "  \n",
    "  precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
    "\n",
    "  for i, (direction, labels) in enumerate(task_set.take(num_batches)):\n",
    "    # We need gradients of model params\n",
    "    with tf.GradientTape() as tape:\n",
    "      # Get model predictions for each image\n",
    "      preds = model(direction)\n",
    "      # Get the log likelihoods of the predictions\n",
    "      ll = tf.nn.log_softmax(preds)\n",
    "    # Attach gradients of ll to ll_grads\n",
    "    ll_grads = tape.gradient(ll, model.trainable_variables)\n",
    "    # Compute F_i as mean of gradients squared\n",
    "    for i, g in enumerate(ll_grads):\n",
    "      precision_matrices[i] += tf.math.reduce_mean(g ** 2, axis=0) / num_batches\n",
    "\n",
    "  return precision_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elastic_penalty(F, theta, theta_A, alpha=1000):\n",
    "  penalty = 0\n",
    "  for i, theta_i in enumerate(theta):\n",
    "    _penalty = tf.math.reduce_sum(F[i] * (theta_i - theta_A[i]) ** 2)\n",
    "    penalty += _penalty\n",
    "  return 0.5*alpha*penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss(labels, preds, model, F, theta_A):\n",
    "  print(\"model.loss:\", model.loss)\n",
    "  loss_b = model.loss(labels, preds)\n",
    "  penalty = compute_elastic_penalty(F, model.trainable_variables, theta_A)\n",
    "  return loss_b + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ewc(model, X_A_train, X_A_test, y_A_train, y_A_test, X_B_train, X_B_test, y_B_train, y_B_test, epochs):\n",
    "  # 모델 생성\n",
    "  LENGTH = 10000 # Packet sequence length\n",
    "  OPTIMIZER = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) # Optimizer\n",
    "  BATCH_SIZE = 128 # Batch size\n",
    "  VERBOSE = 2 # Output display mode\n",
    "  NB_CLASSES = 95 # number of outputs = number of classes\n",
    "  INPUT_SHAPE = (LENGTH,1)\n",
    "\n",
    "  print (\"Building and training DF model\")\n",
    "\n",
    "  model = model.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
    "\n",
    "  model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER,\n",
    "\t  metrics=[\"accuracy\"])\n",
    "  print (\"Model compiled\")\n",
    "\n",
    "  model.fit(X_A_train, y_A_train,\n",
    "\t\tbatch_size=BATCH_SIZE, epochs=epochs,\n",
    "\t\tverbose=VERBOSE)\n",
    "\n",
    "  theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
    "  # We'll only compute Fisher once, you can do it whenever\n",
    "  F = compute_precision_matrices(model, X_A_train, y_A_train, num_batches=128)\n",
    "\n",
    "  print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, X_A_test, y_A_test)))\n",
    "  # Now we set up the training loop for task B with EWC\n",
    "  accuracy = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "  loss = tf.keras.metrics.CategoricalCrossentropy('loss')\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    accuracy.reset_states()\n",
    "    loss.reset_states()\n",
    "\n",
    "    batch_size = 128  # 배치 크기 정의\n",
    "    num_batches = len(X_B_train) // batch_size  # 총 배치 수 계산\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # 배치 데이터 추출\n",
    "        direction = X_B_train[batch * batch_size:(batch + 1) * batch_size]\n",
    "        labels = y_B_train[batch * batch_size:(batch + 1) * batch_size]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 예측값 계산\n",
    "            preds = model(direction)\n",
    "            # EWC 손실 계산\n",
    "            total_loss = ewc_loss(labels, preds, model, F, theta_A)\n",
    "\n",
    "        # 그래디언트 계산\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        # 모델 파라미터 업데이트\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # 손실 및 정확도 업데이트\n",
    "        accuracy.update_state(labels, preds)\n",
    "        loss.update_state(labels, preds)\n",
    "\n",
    "        print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
    "            epoch + 1, batch + 1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end='')\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "  print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, X_B_test, y_B_test)))\n",
    "  print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, X_A_test, y_A_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nAIvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
